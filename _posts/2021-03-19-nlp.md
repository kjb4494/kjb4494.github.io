---
title: Natural Language Processing
author: kjb4494
date: 2021-03-19 13:00:00 +0900
categories: [Machine Learning]
tags: [python, ml]
---

ë°ì´í„°ëŠ” íƒ€ì„ ìŠ¤íƒ¬í”„, ì„¼ì„œ íŒë…ê°’, ì´ë¯¸ì§€, ë²”ì£¼í˜• ë ˆì´ë¸” ë“± ë‹¤ì–‘í•œ í˜•íƒœë¡œ ì œê³µë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í…ìŠ¤íŠ¸ëŠ” ì‚¬ìš© ë°©ë²•ì„ ì•„ëŠ” ì‚¬ëŒë“¤ì—ê²Œ ì—¬ì „íˆ ê°€ì¥ ê·€ì¤‘í•œ ë°ì´í„°ì…ë‹ˆë‹¤.

ìì—°ì–´ ì²˜ë¦¬(NLP)ì— ëŒ€í•œ ì´ ê³¼ì •ì—ì„œëŠ” ìµœê³ ì˜ NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ì¸ spaCyë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì‘ì—…ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ ë‹¤ìŒ ìš©ë„ë¡œ spaCyë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ê¸°ë³¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° íŒ¨í„´ ì¼ì¹˜
- í…ìŠ¤íŠ¸ë¡œ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ êµ¬ì¶•
- ë‹¨ì–´ì™€ ë¬¸ì„œì˜ ì˜ë¯¸ë¥¼ ìˆ˜ì¹˜ì ìœ¼ë¡œ í¬ì°©í•˜ëŠ” ë‹¨ì–´ ì„ë² ë”©(word embeddings)ìœ¼ë¡œ í…ìŠ¤íŠ¸ í‘œí˜„

ì´ ê³¼ì •ì„ ìµœëŒ€í•œ í™œìš©í•˜ë ¤ë©´ ê¸°ê³„ í•™ìŠµì— ëŒ€í•œ ê²½í—˜ì´ í•„ìš”í•©ë‹ˆë‹¤. scikit-learnì— ëŒ€í•œ ê²½í—˜ì´ ì—†ë‹¤ë©´ [Intro to Machine Learning]({% post_url 2021-03-09-intro-to-machine-learning %}) ë° [Intermediate Machine Learning]({% post_url 2021-03-11-intermediate-machine-learning %})ì„ ë¨¼ì € í•™ìŠµí•´ì£¼ì„¸ìš”.

### ëª©ì°¨

> - [_Kaggle Course - Natural Language Processing_](https://www.kaggle.com/learn/natural-language-processing)

1. NLP with spaCy
1. Tokenizing
1. Text preprocessing
1. Pattern Matching
1. Text Classification
1. Word Vectors(Word Embeddings)

---

### NLP with spaCy

spaCyëŠ” NLPë¥¼ ìœ„í•œ ì„ ë„ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë©° ë¹ ë¥´ê²Œ ê°€ì¥ ì¸ê¸°ìˆëŠ” Python í”„ë ˆì„ì›Œí¬ ì¤‘ í•˜ë‚˜ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ì‰½ê²Œ ì§ê´€ì ì´ê³  í›Œë¥­í•œ ë¬¸ì„œë¥¼ ì°¾ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

spaCyëŠ” ì–¸ì–´ë³„ë¡œ ë‹¤ë¥´ë©° í¬ê¸°ê°€ ë‹¤ë¥¸ ëª¨ë¸ì— ì˜ì¡´í•©ë‹ˆë‹¤. spacy.loadë¥¼ ì‚¬ìš©í•˜ì—¬ spaCy ëª¨ë¸ì„ ë¡œë“œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> - spacy ì„¤ì¹˜í•˜ê¸°

    ```shell
    conda install spacy
    ```

> - en ëª¨ë¸ ì„¤ì¹˜í•˜ê¸°

    ```shell
    spacy download en
    ```

ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒì€ ì˜ì–´ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

```python
import spacy

nlp = spacy.load('en_core_web_sm')
```

ëª¨ë¸ì´ë¡œë“œ ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬ í•  ìˆ˜ â€‹â€‹ìˆìŠµë‹ˆë‹¤.

```python
doc = nlp("Tea is healthy and calming, don't you think?")
```

ì´ë ‡ê²Œ ë§Œë“  doc ê°ì²´ë¡œ í•  ìˆ˜ ìˆëŠ” ì¼ì´ ë§ìŠµë‹ˆë‹¤.

---

### Tokenizing

í† í°ì´ í¬í•¨ ëœ dictionary ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. í† í°ì€ ê°œë³„ ë‹¨ì–´ ë° êµ¬ë‘ì ê³¼ ê°™ì€ ë¬¸ì„œì˜ í…ìŠ¤íŠ¸ ë‹¨ìœ„ì…ë‹ˆë‹¤. SpaCyëŠ” "don't"ì™€ ê°™ì€ ì¶•ì•½ì„ "do"ì™€ "n't"ì˜ ë‘ ê°œì˜ í† í°ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. ë°˜ë³µë¬¸ì„ ì´ìš©í•´ í† í°ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
for token in doc:
    print(token)
```

```text
Tea
is
healthy
and
calming
,
do
n't
you
think
?
```

dictionaryë¥¼ ë°˜ë³µí•˜ë©´ í† í° ê°ì²´ê°€ ì œê³µë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ê° í† í°ì—ëŠ” ì¶”ê°€ ì •ë³´ê°€ í•¨ê»˜ ì œê³µë©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì¤‘ìš”í•œ ê²ƒì€ token.lemma\_ ë° token.is_stopì…ë‹ˆë‹¤.

---

### Text preprocessing

ë‹¨ì–´ë¡œ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì„ ê°œì„ í•˜ê¸°ìœ„í•œ ëª‡ ê°€ì§€ ìœ í˜•ì˜ ì „ì²˜ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ëŠ” "lemmatizing"ì…ë‹ˆë‹¤. ë‹¨ì–´ì˜ "ê¸°ë³¸í˜•"ì€ ê¸°ë³¸ í˜•ì‹ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "walk"ëŠ” "walking"ì´ë¼ëŠ” ë‹¨ì–´ì˜ ê¸°ë³¸í˜•ì…ë‹ˆë‹¤. ë”°ë¼ì„œ "walking"ë¼ëŠ” ë‹¨ì–´ë¥¼ lemmatizeí•˜ë©´ "walk"ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ëŠ” ê²ƒë„ ì¼ë°˜ì ì…ë‹ˆë‹¤. ë¶ˆìš©ì–´ëŠ” í•´ë‹¹ ì–¸ì–´ì—ì„œ ìì£¼ ë°œìƒí•˜ë©° ë§ì€ ì •ë³´ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ì…ë‹ˆë‹¤. ì˜ì–´ ë¶ˆìš©ì–´ì—ëŠ” "the", "is", "and", "but", "not"ì´ í¬í•¨ë©ë‹ˆë‹¤.

spaCy í† í°ì„ ì‚¬ìš©í•˜ë©´ token.lemma\_ëŠ” ê¸°ë³¸í˜•ì„ ë°˜í™˜í•˜ê³  token.is_stopì€ í† í°ì´ ë¶ˆìš©ì–´ì´ë©´ Trueë¥¼ ë°˜í™˜í•˜ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
print(f"Token \t\tLemma \t\tStopword".format('Token', 'Lemma', 'Stopword'))
print("-"*40)
for token in doc:
    print(f"{str(token)}\t\t{token.lemma_}\t\t{token.is_stop}")
```

```text
Token 		Lemma 		Stopword
----------------------------------------
Tea		    tea		    False
is		    be		    True
healthy		healthy		False
and		    and		    True
calming		calming		False
,		    ,		    False
do		    do		    True
n't		    n't		    True
you		    you		    True
think		think		False
?		    ?		    False
```

ê¸°ë³¸í˜•ê³¼ ë¶ˆìš©ì–´ ì‹ë³„ì´ ì¤‘ìš”í•œ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”? ì–¸ì–´ ë°ì´í„°ì—ëŠ” ìœ ìµí•œ ë°ì´í„°ì™€ í˜¼í•©ëœ ë§ì€ ë…¸ì´ì¦ˆê°€ ìˆìŠµë‹ˆë‹¤. ìœ„ ë¬¸ì¥ì—ì„œ ì¤‘ìš”í•œ ë‹¨ì–´ëŠ” tea, healthy, calmingì…ë‹ˆë‹¤. ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ë©´ ì˜ˆì¸¡ ëª¨ë¸ì´ ê´€ë ¨ ë‹¨ì–´ì— ì§‘ì¤‘í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë™ì¼í•œ ë‹¨ì–´ì˜ ì—¬ëŸ¬ í˜•íƒœë¥¼ í•˜ë‚˜ì˜ ê¸°ë³¸ í˜•íƒœë¡œ ê²°í•©í•¨ìœ¼ë¡œì¨ ìœ ì‚¬í•˜ê²Œ í‘œì œí™”í•˜ëŠ” ê²ƒì´ ë„ì›€ì´ë©ë‹ˆë‹¤("calming", "calms", "calmed"ëŠ” ëª¨ë‘ "calm"ìœ¼ë¡œ ë³€ê²½ë©ë‹ˆë‹¤).

ê·¸ëŸ¬ë‚˜ ë¶ˆìš©ì–´ë¥¼ lemmatizingí•˜ê³  ì‚­ì œí•˜ë©´ ëª¨ë¸ ì„±ëŠ¥ì´ ì €í•˜ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ì „ì²˜ë¦¬ë¥¼ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ìµœì í™” í”„ë¡œì„¸ìŠ¤ì˜ ì¼ë¶€ë¡œ ì·¨ê¸‰í•´ì•¼í•©ë‹ˆë‹¤.

---

### Pattern Matching

ë˜ ë‹¤ë¥¸ ì¼ë°˜ì ì¸ NLP ì‘ì—…ì€ í…ìŠ¤íŠ¸ì˜ ì²­í¬(chunks) ë˜ëŠ” ì „ì²´ ë¬¸ì„œ ë‚´ì—ì„œ í† í° ë˜ëŠ” êµ¬ë¬¸ì„ ë§¤ì¹­ì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì •ê·œì‹ìœ¼ë¡œë„ íŒ¨í„´ ë§¤ì¹­ì„ ìˆ˜í–‰ í•  ìˆ˜ ìˆì§€ë§Œ spaCyì˜ ë§¤ì¹­ ê¸°ëŠ¥ì´ ë” ì‚¬ìš©í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.

ê°œë³„ í† í°ì„ ë§¤ì¹­ ì‹œí‚¤ë ¤ë©´ Matcherë¥¼ ë§Œë“­ë‹ˆë‹¤. term ëª©ë¡ì„ ë§¤ì¹­ ì‹œí‚¤ë ¤ë©´ PhraseMatcherë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì‰½ê³  íš¨ìœ¨ì ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì¼ë¶€ í…ìŠ¤íŠ¸ì—ì„œ ë‹¤ì–‘í•œ ìŠ¤ë§ˆíŠ¸ í° ëª¨ë¸ì´ í‘œì‹œë˜ëŠ” ìœ„ì¹˜ë¥¼ ì°¾ìœ¼ë ¤ë©´ ê´€ì‹¬ìˆëŠ” ëª¨ë¸ ì´ë¦„ì— ëŒ€í•œ íŒ¨í„´ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € PhraseMatcher ìì²´ë¥¼ ë§Œë“­ë‹ˆë‹¤.

```python
from spacy.matcher import PhraseMatcher
matcher = PhraseMatcher(nlp.vocab, attr='LOWER')
```

matcherëŠ” ëª¨ë¸ì˜ ë‹¨ì–´(vocabulary)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒì„±ë©ë‹ˆë‹¤. ì—¬ê¸°ì—ì„œëŠ” ì´ì „ì— ë¡œë“œ í•œ ì‘ì€ ì˜ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. attr='LOWER'ë¥¼ ì„¤ì •í•˜ë©´ ì†Œë¬¸ì í…ìŠ¤íŠ¸ì˜ êµ¬ë¬¸ê³¼ ë§¤ì¹­í•©ë‹ˆë‹¤. ì´ê²ƒì€ ëŒ€ì†Œ ë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠëŠ” ë§¤ì¹­ì„ ì œê³µí•©ë‹ˆë‹¤.

ë‹¤ìŒìœ¼ë¡œ í…ìŠ¤íŠ¸ì—ì„œ ë§¤ì¹­ì‹œí‚¬ ìš©ì–´(term) ëª©ë¡ì„ ë§Œë“­ë‹ˆë‹¤. êµ¬ë¬¸ ë§¤ì¹­ì—ëŠ” ë¬¸ì„œ(document) ê°ì²´ë¡œì„œì˜ íŒ¨í„´ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¥¼ ì–»ëŠ” ê°€ì¥ ì‰¬ìš´ ë°©ë²•ì€ nlp ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

```python
terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']
patterns = [nlp(text) for text in terms]
matcher.add("TerminologyList", patterns)
```

ê·¸ëŸ° ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ documentë¥¼ ë§Œë“¤ì–´ ê²€ìƒ‰í•˜ê³  êµ¬ë¬¸ matcherë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ termì´ ë‚˜ì˜¤ëŠ” ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

```python
# Borrowed from https://daringfireball.net/linked/2019/09/21/patel-11-pro
text_doc = nlp("Glowing review overall, and some really interesting side-by-side "
               "photography tests pitting the iPhone 11 Pro against the "
               "Galaxy Note 10 Plus and last yearâ€™s iPhone XS and Google Pixel 3.")
matches = matcher(text_doc)
print(matches)
```

```text
[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]
```

ì—¬ê¸°ì„œ ë§¤ì¹­ë˜ëŠ” í•­ëª©ì€ match IDì˜ tupleê³¼ êµ¬ì˜ ì‹œì‘ê³¼ ëì˜ ìœ„ì¹˜ì…ë‹ˆë‹¤.

```python
match_id, start, end = matches[0]
print(nlp.vocab.strings[match_id], text_doc[start:end])
```

```text
TerminologyList iPhone 11
```

---

### Text Classification

#### Text Classification with SpaCy

NLPì˜ ì¼ë°˜ì ì¸ ì‘ì—…ì€ í…ìŠ¤íŠ¸ ë¶„ë¥˜ì…ë‹ˆë‹¤. ì´ê²ƒì€ ê¸°ì¡´ì˜ ê¸°ê³„ í•™ìŠµ ì˜ë¯¸ì—ì„œì˜ "ë¶„ë¥˜"ì´ë©° í…ìŠ¤íŠ¸ì— ì ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìŠ¤íŒ¸ ê°ì§€, ê°ì • ë¶„ì„ ë° ê³ ê° ì¿¼ë¦¬ íƒœê·¸ ì§€ì •ì´ ìˆìŠµë‹ˆë‹¤.

ë¶„ë¥˜ê¸°(classifier)ëŠ” ëŒ€ë¶€ë¶„ì˜ ì´ë©”ì¼ í´ë¼ì´ì–¸íŠ¸ì—ì„œ ê³µí†µì ì¸ ê¸°ëŠ¥ì¸ ìŠ¤íŒ¸ ë©”ì‹œì§€ë¥¼ ê°ì§€í•©ë‹ˆë‹¤. ë‹¤ìŒì€ ì‚¬ìš©í•  ë°ì´í„°ì— ëŒ€í•œ ê°œìš”ì…ë‹ˆë‹¤.

> - [_spam.csv_](https://www.kaggle.com/matleonard/nlp-course?select=spam.csv)

```python
import pandas as pd

# Loading the spam data
# ham is the label for non-spam messages
spam = pd.read_csv('input_data/spam.csv')
spam.head(10)
```

```text
  label                                               text
0   ham  Go until jurong point, crazy.. Available only ...
1   ham                      Ok lar... Joking wif u oni...
2  spam  Free entry in 2 a wkly comp to win FA Cup fina...
3   ham  U dun say so early hor... U c already then say...
4   ham  Nah I don't think he goes to usf, he lives aro...
5  spam  FreeMsg Hey there darling it's been 3 week's n...
6   ham  Even my brother is not like to speak with me. ...
7   ham  As per your request 'Melle Melle (Oru Minnamin...
8  spam  WINNER!! As a valued network customer you have...
9  spam  Had your mobile 11 months or more? U R entitle...
```

#### Bag of Words

ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì€ ì›ì‹œ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ í•™ìŠµí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¦‰, í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•´ì•¼í•©ë‹ˆë‹¤.

ê°€ì¥ ê°„ë‹¨í•œ ì¼ë°˜ì ì¸ í‘œí˜„ì€ ì›-í•« ì¸ì½”ë”©ì˜ ë³€í˜•ì…ë‹ˆë‹¤. ê° documentë¥¼ vocabularyì˜ ê° termì— ëŒ€í•œ ì‚¬ìš© ë¹ˆë„ì— ë”°ë¥¸ ë²¡í„°ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. vocabularyëŠ” documentì˜ ëª¨ìŒì¸ ë§ë­‰ì¹˜(corpus)ì˜ ëª¨ë“  í† í°(terms)ì—ì„œ êµ¬ì„±ë©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, "Tea is life. Tea is love." "Tea is healthy, calming, and delicious."ê°€ corpusë¼ë©´, ê·¸ëŸ¬ë©´ vocabularyëŠ” {"tea", "is", "life", "love", "healthy", "calming", "and", "delicious"}(êµ¬ë‘ì  ë¬´ì‹œ)ì…ë‹ˆë‹¤.

ê° ë¬¸ì„œì— ëŒ€í•´ termì´ ë°œìƒí•˜ëŠ” íšŸìˆ˜ë¥¼ ì„¸ê³  í•´ë‹¹ ê³„ìˆ˜ë¥¼ ë²¡í„°ì˜ ì ì ˆí•œ ìš”ì†Œì— ë°°ì¹˜í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ë¬¸ì¥ì€ "tea"ë¥¼ ë‘ ë²ˆ ê°€ì§€ê³  ìˆê³  ê·¸ê²ƒì€ vocabularyì˜ ì²« ë²ˆì§¸ ìœ„ì¹˜ì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ë²¡í„°ì˜ ì²« ë²ˆì§¸ ìš”ì†Œì— ìˆ«ì 2ë¥¼ ë„£ìŠµë‹ˆë‹¤. ë²¡í„°ë¡œì„œì˜ ë¬¸ì¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

```text
ğ‘£1 = [22110000]
ğ‘£2 = [11001111]
```

ì´ê²ƒì„ Bag of Wordsì´ë¼ê³ í•©ë‹ˆë‹¤. ìœ ì‚¬í•œ termì„ ê°€ì§„ documentëŠ” ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ê°€ì§€ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Vocabularyì—ëŠ” ì¢…ì¢… ìˆ˜ë§Œ ê°œì˜ ìš©ì–´ê°€ ìˆìœ¼ë¯€ë¡œ ì´ëŸ¬í•œ ë²¡í„°ëŠ” ë§¤ìš° ì»¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë˜ ë‹¤ë¥¸ ì¼ë°˜ì ì¸ í‘œí˜„ì€ TF-IDF(Term Frequency-Inverse Document Frequency)ì…ë‹ˆë‹¤. TF-IDFëŠ” ê° termì˜ ê°œìˆ˜ê°€ corpusì—ì„œ termì˜ ë¹ˆë„ì— ë”°ë¼ ì¡°ì •ëœë‹¤ëŠ” ì ì„ ì œì™¸í•˜ë©´ bag of wordsì™€ ìœ ì‚¬í•©ë‹ˆë‹¤. TF-IDFë¥¼ ì‚¬ìš©í•˜ë©´ ì ì¬ì ìœ¼ë¡œ ëª¨ë¸ì„ ê°œì„  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ ë‚´ìš©ì€ ì—¬ê¸°ì—ì„œëŠ” ë‹¤ë£¨ì§€ ì•ŠìŠµë‹ˆë‹¤.

#### Building a Bag of Words model

bag of wordsì— documentê°€ ìˆìœ¼ë©´ ì´ëŸ¬í•œ ë²¡í„°ë¥¼ ëª¨ë“  ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì— ëŒ€í•œ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. spaCyëŠ” bag of words ë³€í™˜ì„ ì²˜ë¦¬í•˜ê³  TextCategorizer í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ì„ í˜• ëª¨ë¸(linear model)ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

TextCategorizerëŠ” spaCy íŒŒì´í”„(pipe)ì…ë‹ˆë‹¤. íŒŒì´í”„ëŠ” í† í°ì„ ì²˜ë¦¬í•˜ê³  ë³€í™˜í•˜ê¸°ìœ„í•œ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. nlp=spacy.load('en*core_web_sm')ì„ ì‚¬ìš©í•˜ì—¬ spaCy ëª¨ë¸ì„ ìƒì„± í•  ë•Œ ìŒì„± íƒœê¹…, ì—”í‹°í‹° ì¸ì‹ ë° ê¸°íƒ€ ë³€í™˜ì˜ ì¼ë¶€ë¥¼ ìˆ˜í–‰í•˜ëŠ” ê¸°ë³¸ íŒŒì´í”„ê°€ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ doc=nlp( "Some text here")ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ë©´ íŒŒì´í”„ì˜ ì¶œë ¥ì´ doc ê°ì²´ì˜ í† í°ì— ì²¨ë¶€ë©ë‹ˆë‹¤. token.lemma*ì˜ ê¸°ë³¸í˜•ì€ ì´ íŒŒì´í”„ ì¤‘ í•˜ë‚˜ì—ì„œ ë‚˜ì˜µë‹ˆë‹¤.

ëª¨ë¸ì— íŒŒì´í”„ë¥¼ ì œê±°í•˜ê±°ë‚˜ ì¶”ê°€ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ í•  ì¼ì€ íŒŒì´í”„ì—†ì´ ë¹ˆ ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤(ëª¨ë“  ëª¨ë¸ì—ëŠ” í•­ìƒ tokenizerê°€ ìˆìœ¼ë¯€ë¡œ tokenizerëŠ” ì œì™¸). ê·¸ëŸ° ë‹¤ìŒ TextCategorizer íŒŒì´í”„ë¥¼ ë§Œë“¤ì–´ ë¹ˆ ëª¨ë¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.

```python
# Create an empty model
from spacy.lang.en import English
nlp = English()

# Create the TextCategorizer with exclusive classes and "bow" architecture
# https://spacy.io/api/language#create_pipe
# https://spacy.io/api/architectures#TextCatBOW
nlp.create_pipe(
    "textcat",
    config={
        'model': {
            '@architectures': 'spacy.TextCatBOW.v1',
            'exclusive_classes': True,
            'ngram_size': 1,
            'no_output_layer': False
        }
    }
)

textcat = nlp.add_pipe('textcat')
```

í´ë˜ìŠ¤ê°€ ham ë˜ëŠ” spamì´ë¯€ë¡œ "exclusive_classes"ë¥¼ Trueë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ë˜í•œ bag of words("bow") ì•„í‚¤í…ì²˜ë¡œ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤. spaCyëŠ” CNN(convolutional neural network) ì•„í‚¤í…ì²˜ë„ ì œê³µí•˜ì§€ë§Œ í˜„ì¬ í•„ìš”í•œ ê²ƒë³´ë‹¤ ë” ë³µì¡í•©ë‹ˆë‹¤.

ë‹¤ìŒìœ¼ë¡œ ëª¨ë¸ì— ë ˆì´ë¸”ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ "ham"ì€ ì‹¤ì œ ë©”ì‹œì§€ë¥¼, "spam"ì€ ìŠ¤íŒ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤.

```python
# Add labels to text classifier
textcat.add_label("ham")
textcat.add_label("spam")
```

#### Training a Text Categorizer Model

ë‹¤ìŒìœ¼ë¡œ ë°ì´í„°ì˜ ë ˆì´ë¸”ì„ TextCategorizerì— í•„ìš”í•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ê° ë¬¸ì„œì— ëŒ€í•´ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ boolean ê°’ì˜ dictionaryë¥¼ ë§Œë“­ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´ í…ìŠ¤íŠ¸ê°€ "ham"ì´ë©´ ì‚¬ì „ {'ham': True, 'spam': False}ì´ í•„ìš”í•©ë‹ˆë‹¤. ëª¨ë¸ì€ 'cats' keyê°€ ìˆëŠ” ë‹¤ë¥¸ dictionary ë‚´ì—ì„œ ì´ëŸ¬í•œ ë ˆì´ë¸”ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤.

```python
train_data = []
for item in spam.iloc:
    doc = nlp.make_doc(item.text)
    example = Example.from_dict(
        doc,
        {
            'cats': {
                'ham': item.label == 'ham',
                'spam': item.label == 'spam'
            }
        }
    )
    train_data.append(example)
```

ì´ì œ ëª¨ë¸ì„ í›ˆë ¨ í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. ë¨¼ì € nlp.begin_training()ì„ ì‚¬ìš©í•˜ì—¬ ìµœì í™” í”„ë¡œê·¸ë¨ì„ ë§Œë“­ë‹ˆë‹¤. spaCyëŠ”ì´ ìµœì í™” í”„ë¡œê·¸ë¨ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ì„ ì‘ì€ ë°°ì¹˜(batch)ë¡œ í›ˆë ¨í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì…ë‹ˆë‹¤. spaCyëŠ” í›ˆë ¨ì„ ìœ„í•´ ë¯¸ë‹ˆ ë°°ì¹˜(minibatch)ë¥¼ ìƒì„±í•˜ëŠ” ìƒì„±ê¸°ë¥¼ ë°˜í™˜í•˜ëŠ” ë¯¸ë‹ˆ ë°°ì¹˜ í•¨ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ë¯¸ë‹ˆ ë°°ì¹˜ëŠ” í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸”ë¡œ ë¶„í•  ëœ ë‹¤ìŒ nlp.updateì™€ í•¨ê»˜ ì‚¬ìš©ë˜ì–´ ëª¨ë¸ì˜ ë§¤ê°œ ë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

```python
random.seed(1)
spacy.util.fix_random_seed(1)
optimizer = nlp.begin_training()

# Iterate through minibatches
# https://spacy.io/api/language#update
losses = {}
for epoch in range(10):
    random.shuffle(train_data)
    # Create the batch generator with batch size = 8
    batches = minibatch(train_data, size=8)
    # Iterate through minibatches
    for batch in batches:
        nlp.update(batch, sgd=optimizer, losses=losses)
    print(losses)
```

```text
{'textcat': 0.5549327614952294}
{'textcat': 0.6942927120308386}
{'textcat': 0.7603908179353354}
{'textcat': 0.7798789901942472}
{'textcat': 0.8150759574802218}
{'textcat': 0.8868370061354872}
{'textcat': 0.9019728956993387}
{'textcat': 0.9116310472891854}
{'textcat': 0.9546235444465657}
{'textcat': 0.9629162331118234}
```

#### Making Predictions

ì´ì œ í•™ìŠµ ëœ ëª¨ë¸ì´ ìˆìœ¼ë¯€ë¡œ predict() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì…ë ¥ í…ìŠ¤íŠ¸ëŠ” nlp.tokenizerë¡œ í† í°í™”í•´ì•¼í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì ìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” predict ë©”ì„œë“œì— í† í°ì„ ì „ë‹¬í•©ë‹ˆë‹¤. ì ìˆ˜ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ê°€ í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì…ë‹ˆë‹¤.

```python
texts = ["Are you ready for the tea party????? It's gonna be wild",
         "URGENT Reply to this message for GUARANTEED FREE TEA" ]
docs = [nlp.make_doc(text) for text in texts]

# Use textcat to get the scores for each doc
textcat = nlp.get_pipe('textcat')
scores = textcat.predict(docs)

scores
```

```text
array([[9.9998641e-01, 1.3548736e-05],
       [3.4928420e-03, 9.9650711e-01]], dtype=float32)
```

ì ìˆ˜ëŠ” í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ë ˆì´ë¸”ì„ ì„ íƒí•˜ì—¬ ë‹¨ì¼ í´ë˜ìŠ¤ ë˜ëŠ” ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. scores.argmaxë¡œ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ì¸ë±ìŠ¤ë¥¼ ì–»ì€ ë‹¤ìŒ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ textcat.labelsì—ì„œ ë ˆì´ë¸” ë¬¸ìì—´ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.

```python
# From the scores, find the label with the highest score/probability
predicted_labels = scores.argmax(axis=1)
[textcat.labels[label] for label in predicted_labels]
```

```text
['ham', 'spam']
```

ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ê²ƒì€ ì¼ë‹¨ ì˜ˆì¸¡ì´ ì´ë£¨ì–´ì§€ë©´ ê°„ë‹¨í•©ë‹ˆë‹¤. ì •í™•ë„ë¥¼ ì¸¡ì •í•˜ë ¤ë©´ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì˜¬ë°”ë¥¸ ì˜ˆì¸¡ì´ ëª‡ ê°œ ì´ë£¨ì–´ ì¡ŒëŠ”ì§€ ê³„ì‚°í•˜ê³  ì´ ì˜ˆì¸¡ ìˆ˜ë¡œ ë‚˜ëˆˆ ê°’ì…ë‹ˆë‹¤.

---

### Word Vectors(Word Embeddings)

ì´ ì‹œì ì—ì„œ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ê¸°ê³„ í•™ìŠµì€ ë¨¼ì € í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ í‘œí˜„í•´ì•¼í•œë‹¤ëŠ” ê²ƒì„ ì•Œê³  ìˆìŠµë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ bag of words í‘œí˜„ìœ¼ë¡œ ì´ ì‘ì—…ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¼ë°˜ì ìœ¼ë¡œ ë‹¨ì–´ ì„ë² ë”©ìœ¼ë¡œ ë” ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹¨ì–´ ì„ë² ë”©(ë‹¨ì–´ ë²¡í„°ë¼ê³ ë„ í•¨)ì€ ë²¡í„°ê°€ í•´ë‹¹ ë‹¨ì–´ê°€ ì‚¬ìš©ë˜ëŠ” ë°©ì‹ ë˜ëŠ” ì˜ë¯¸ì— í•´ë‹¹í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê° ë‹¨ì–´ë¥¼ ìˆ«ìë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë²¡í„° ì¸ì½”ë”©ì€ ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚˜ëŠ” ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•˜ì—¬ í•™ìŠµë©ë‹ˆë‹¤. ìœ ì‚¬í•œ ë¬¸ë§¥ì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ëŠ” ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ê°–ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, "í‘œë²”", "ì‚¬ì" ë° "í˜¸ë‘ì´"ì— ëŒ€í•œ ë²¡í„°ëŠ” ì„œë¡œ ê°€ê¹ì§€ë§Œ "í–‰ì„±"ê³¼ "ì„±"ì€ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.

ë” ë©‹ì§„ ë‹¨ì–´ ì‚¬ì´ì˜ ê´€ê³„ëŠ” ìˆ˜í•™ ì—°ì‚°ìœ¼ë¡œ ì¡°ì‚¬ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. "man"ê³¼ "woman"ì— ëŒ€í•œ ë²¡í„°ë¥¼ ë¹¼ë©´ ë‹¤ë¥¸ ë²¡í„°ê°€ ë°˜í™˜ë©ë‹ˆë‹¤. "king"ì— ëŒ€í•œ ë²¡í„°ì— ì¶”ê°€í•˜ë©´ ê²°ê³¼ëŠ” "queen"ì— ëŒ€í•œ ë²¡í„°ì— ê°€ê¹ìŠµë‹ˆë‹¤.

![ì‚¬ì§„](/assets/img/posts/legacy/ml/nlp-001.png)

ì´ëŸ¬í•œ ë²¡í„°ëŠ” ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì˜ í”¼ì³ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨ì–´ ë²¡í„°ëŠ” ì¼ë°˜ì ìœ¼ë¡œ bag of words ì¸ì½”ë”© ìœ„ì˜ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. spaCyëŠ” Word2Vecì´ë¼ëŠ” ëª¨ë¸ì—ì„œ í•™ìŠµí•œ ì„ë² ë”©ì„ ì œê³µí•©ë‹ˆë‹¤. en_core_web_lgì™€ ê°™ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ì•¡ì„¸ìŠ¤ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ .vector ì†ì„±ì˜ í† í°ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
import numpy as np
import spacy

# Need to load the large model to get the vectors
nlp = spacy.load('en_core_web_lg')
```

```python
# Disabling other pipes because we don't need them and it'll speed up this part a bit
text = "These vectors can be used as features for machine learning models."
with nlp.disable_pipes():
    vectors = np.array([token.vector for token in  nlp(text)])
```

```python
vectors.shape
```

```text
(12, 300)
```

ì´ê²ƒì€ ê° ë‹¨ì–´ì— ëŒ€í•´ í•˜ë‚˜ì˜ ë²¡í„°ê°€ ìˆëŠ” 300ì°¨ì› ë²¡í„°ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ëŠ” ë¬¸ì„œ ìˆ˜ì¤€(document-level) ë ˆì´ë¸”ë§Œ ê°€ì§€ê³  ìˆìœ¼ë©° ëª¨ë¸ì€ ë‹¨ì–´ ìˆ˜ì¤€(word-level) ì„ë² ë”©ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì „ì²´ ë¬¸ì„œì— ëŒ€í•œ ë²¡í„° í‘œí˜„ì´ í•„ìš”í•©ë‹ˆë‹¤.

ëª¨ë“  ë‹¨ì–´ ë²¡í„°ë¥¼ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‹¨ì¼ ë¬¸ì„œ ë²¡í„°ë¡œ ê²°í•©í•˜ëŠ” ë°©ë²•ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. ê°„ë‹¨í•˜ê³  ë†€ëë„ë¡ íš¨ê³¼ì ì¸ ë°©ë²•ì€ ë‹¨ìˆœíˆ ë¬¸ì„œì˜ ê° ë‹¨ì–´ì— ëŒ€í•œ ë²¡í„°ë¥¼ í‰ê· í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ëŸ¬í•œ ë¬¸ì„œ ë²¡í„°ë¥¼ ëª¨ë¸ë§ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

spaCyëŠ” doc.vectorë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” í‰ê·  ë¬¸ì„œ ë²¡í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ë‹¤ìŒì€ ìŠ¤íŒ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ë¬¸ì„œ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤.

```python
import pandas as pd

# Loading the spam data
# ham is the label for non-spam messages
spam = pd.read_csv('../input/nlp-course/spam.csv')

with nlp.disable_pipes():
    doc_vectors = np.array([nlp(text).vector for text in spam.text])

doc_vectors.shape
```

```text
(5572, 300)
```

#### Classification Models

ë¬¸ì„œ ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ë©´ scikit-learn ëª¨ë¸, xgboost ëª¨ë¸ ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸ë§ì— ëŒ€í•œ ê¸°íƒ€ í‘œì¤€ ì ‘ê·¼ ë°©ì‹ì„ í•™ìŠµ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    doc_vectors, spam.label, test_size=0.1, random_state=1
)
```

ë‹¤ìŒì€ [_SVM(Support Vector Machine)_](https://scikit-learn.org/stable/modules/svm.html#svm)ì„ ì‚¬ìš©í•˜ëŠ” ì˜ˆì…ë‹ˆë‹¤. Scikit-learnì€ SVM ë¶„ë¥˜ê¸°ì¸ LinearSVCë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ê²ƒì€ ë‹¤ë¥¸ scikit-learn ëª¨ë¸ê³¼ ìœ ì‚¬í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤.

```python
from sklearn.svm import LinearSVC

# Set dual=False to speed up training, and it's not needed
svc = LinearSVC(random_state=1, dual=False, max_iter=10000)
svc.fit(X_train, y_train)
print(f"Accuracy: {svc.score(X_test, y_test) * 100:.3f}%", )
```

```text
Accuracy: 97.849%
```

#### Document Similarity

ì½˜í…ì¸ ê°€ ìœ ì‚¬í•œ ë¬¸ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë²¡í„° ê°„ì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ì—¬ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì— ëŒ€í•œ ì¼ë°˜ì ì¸ ë©”íŠ¸ë¦­ì€ ë‘ ë²¡í„° ğšì™€ ğ› ì‚¬ì´ì˜ ê°ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì…ë‹ˆë‹¤.

```text
cosğœƒ = ğšâ‹…ğ› / â€–ğšâ€– â€–ğ›â€–
```

ì´ê²ƒì€ ğšê³¼ ğ›ì˜ ë‚´ì ì„ ê° ë²¡í„°ì˜ í¬ê¸°ë¡œ ë‚˜ëˆˆ ê°’ì…ë‹ˆë‹¤. ì½”ì‚¬ì¸ ìœ ì‚¬ì„±ì€ -1ê³¼ 1 ì‚¬ì´ì—ì„œ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìœ¼ë©°, ê°ê° ì™„ì „ ìœ ì‚¬ì„±ê³¼ ì™„ì „íˆ ë°˜ëŒ€ì…ë‹ˆë‹¤. ì´ë¥¼ ê³„ì‚°í•˜ë ¤ë©´ scikit-learnì˜ ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ìì²´ í•¨ìˆ˜ë¥¼ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
def cosine_similarity(a, b):
    return a.dot(b)/np.sqrt(a.dot(a) * b.dot(b))
```

```python
a = nlp("REPLY NOW FOR FREE TEA").vector
b = nlp("According to legend, Emperor Shen Nung discovered tea when leaves from a wild tree blew into his pot of boiling water.").vector
cosine_similarity(a, b)
```

```text
0.7030031
```
